{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "Archived code for computing missing and hallucinated entities within GPT's labels."
      ],
      "metadata": {
        "id": "-yW2CcMR4su3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAcY_Hnk4TUT"
      },
      "source": [
        "# Env Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8CskFMP2VIU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import csv\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5ilhGuR28IC",
        "outputId": "c83842f4-abcb-4274-e736-1b86dce85888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive/'6.8611 Research Project'/'Colab Notebooks'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nH2yq8Gq3Aq_",
        "outputId": "88747ab7-80f2-4af1-9909-c71d54232b89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/6.8611 Research Project/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70AoigM1u6Ct"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the datasets into dataframes\n",
        "def load_tsv_dataset(file_path):\n",
        "  \"\"\"\n",
        "  Loads a tsv dataset. Renames thne columns to 'token' and 'label'.\n",
        "  Note that renaming the columns will overwrite the first row of the dataframe\n",
        "  \"\"\"\n",
        "  df = pd.read_csv(file_path, delimiter='\\t', header=None, engine='python')\n",
        "  df.columns = ['token', 'label']\n",
        "  return df\n",
        "\n",
        "def load_csv_dataset(file_path):\n",
        "  \"\"\"\n",
        "  Loads a csv dataset. Renames thne columns to 'token' and 'label'.\n",
        "  Note that renaming the columns will overwrite the first row of the dataframe\n",
        "  \"\"\"\n",
        "  df = pd.read_csv(file_path, delimiter=',', header=None, engine='python')\n",
        "  df.columns = ['token', 'label']\n",
        "  return df\n",
        "\n",
        "def split_by_sentence(list_of_strings):\n",
        "  sentences = []\n",
        "  current_sentence = []\n",
        "  num_sentences = 0\n",
        "\n",
        "  for word in list_of_strings:\n",
        "      current_sentence.append(word)\n",
        "      if type(word) is str and word.endswith('.'):\n",
        "          num_sentences += 1\n",
        "          sentence_str = ' '.join(map(str, current_sentence))\n",
        "          sentences.append(sentence_str)\n",
        "          current_sentence = []\n",
        "\n",
        "  print(\"\\nNumber of sentences: \", num_sentences)\n",
        "  return sentences\n",
        "\n",
        "def get_filtered_entities(df, target_label):\n",
        "  \"\"\"\n",
        "  df (pandas dataframe): has two columns 'token' and 'label'\n",
        "  target_label: 'B', 'I', or 'O' (see description above for what these signify)\n",
        "\n",
        "  Filtering involves: removing blanks, and filtering out entities that consist\n",
        "  only of punctuation, numbers, or single letters.\n",
        "\n",
        "  Return a frequency of all filtered entities with label 'target_label'.\n",
        "  \"\"\"\n",
        "  filtered_df = df[df['label'] == target_label]\n",
        "  target_entities = filtered_df['token'].tolist() # a set of all the entities with the target label\n",
        "\n",
        "  # regex for filtering out nonsense strings\n",
        "  punctuation = re.escape(string.punctuation)\n",
        "  pattern = re.compile(rf'^(?![a-zA-Z]?$)(?!\\d+$)(?!^[{punctuation}]+$).+')\n",
        "  target_entities = [ent for ent in target_entities if pattern.match(ent)]\n",
        "  return Counter(target_entities)"
      ],
      "metadata": {
        "id": "hiBRh3Ht2ZdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adjust dataset_name, num_shots, file_name_end\n",
        "dataset_name = \"NCBI\" # POSSIBILITIES: ['NCBI', 'JNLPBA', 'BC5CDR-C', 'BC4CDR-D', 'BC2GM']\n",
        "num_shots = \"few_shot\" # POSSIBILITIES: ['zero_shot', 'one_shot', 'few_shot']\n",
        "file_name_end = \"-devel.csv\"\n",
        "\n",
        "all_tokens = load_tsv_dataset(\"llm-annotations/datasets/\"+dataset_name+\"/devel.tsv\")['token'].tolist()\n",
        "all_labels = load_tsv_dataset(\"llm-annotations/datasets/\"+dataset_name+\"/devel.tsv\")['label'].tolist()\n",
        "llm_devel_tokens = load_csv_dataset(\"devel_gpt_generated_datasets/\"+num_shots+\"/\"+dataset_name+file_name_end)['token'].tolist()\n",
        "llm_devel_labels = load_csv_dataset(\"devel_gpt_generated_datasets/\"+num_shots+\"/\"+dataset_name+file_name_end)['label'].tolist()"
      ],
      "metadata": {
        "id": "zSDvp3mq2cC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load_<file_type>_dataset's dataframe reads null as nan, so we convert nan back to null\n",
        "for i in range(len(all_tokens)):\n",
        "    token = all_tokens[i]\n",
        "    if isinstance(token,float) and str(token)==\"nan\":\n",
        "        all_tokens[i] = \"null\"\n",
        "\n",
        "for i in range(len(llm_devel_tokens)):\n",
        "    token = llm_devel_tokens[i]\n",
        "    if isinstance(token,float) and str(token)==\"nan\":\n",
        "        llm_devel_tokens[i] = \"null\""
      ],
      "metadata": {
        "id": "T8d_Mjzd4CML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset: \"+dataset_name)\n",
        "print(\"# shots: \"+num_shots)\n",
        "print(\"# tokens in original devel dataset: \"+str(len(all_tokens)))\n",
        "print(\"# tokens in LLM-generated dataset: \"+str(len(llm_devel_tokens)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdWkiZHf4OdY",
        "outputId": "be21d5db-04ae-4086-cac0-d1b82dca24b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: NCBI-disease\n",
            "# shots: few_shot\n",
            "# tokens in original devel dataset: 23959\n",
            "# tokens in LLM-generated dataset: 23917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Entities\n",
        "An entity is a sequence of tokens starting with a token labeled as “B” and ends at the last consecutive token labeled as “I” (the token after this token is either labeled as “O” or the last token labeled “I” is the last token in the dataset). For intrinsic evaluation, we filtered for all the unique entities in the original devel and LLM-generated datasets."
      ],
      "metadata": {
        "id": "5fFXo9bV5LZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add unique entities in original devel dataset to a set\n",
        "\n",
        "entities_original = set()\n",
        "i = 0\n",
        "while i<len(all_tokens):\n",
        "    current_entity = \"\"\n",
        "    if (all_labels[i]==\"B\"):\n",
        "        current_entity+=all_tokens[i]+\" \"\n",
        "        i+=1\n",
        "        while (all_labels[i] == \"I\"):\n",
        "            current_entity+=all_tokens[i]+\" \"\n",
        "            i+=1\n",
        "        current_entity = current_entity[:len(current_entity)-1]\n",
        "        entities_original.add(current_entity)\n",
        "        current_entity = \"\"\n",
        "    else:\n",
        "        #if (all_labels[i]==\"I\"):\n",
        "            #print(\"unexpected label I without B before it? or label that's not B, I, or O\")\n",
        "            #print(\"token: \"+all_tokens[i])\n",
        "            #print(\"label: \"+all_labels[i])\n",
        "            #print(\"row: \"+str(i+1))\n",
        "        i+=1\n",
        "\n",
        "print(\"# unique entities in original dataset: \"+str(len(entities_original)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zj2kySn4UVh",
        "outputId": "d2ca0011-bd1b-43d2-d2e7-2e8b6c4204f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# unique entities in original dataset: 363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add unique entities in LLM-generated dataset to a set\n",
        "entities_llm = set()\n",
        "i = 0\n",
        "while i<len(llm_devel_tokens):\n",
        "    current_entity = \"\"\n",
        "    if (llm_devel_labels[i]==\"B\"):\n",
        "        current_entity+=llm_devel_tokens[i]+\" \"\n",
        "        i+=1\n",
        "        while (llm_devel_labels[i] == \"I\"):\n",
        "            current_entity+=llm_devel_tokens[i]+\" \"\n",
        "            i+=1\n",
        "        current_entity = current_entity[:len(current_entity)-1]\n",
        "        entities_llm.add(current_entity)\n",
        "        current_entity = \"\"\n",
        "    else:\n",
        "        #if (llm_devel_labels[i]==\"I\"):\n",
        "            #print(\"unexpected label I without B before it? or label that's not B, I, or O\")\n",
        "            #print(\"llm token: \"+llm_devel_tokens[i])\n",
        "            #print(\"llm label: \"+llm_devel_labels[i])\n",
        "            #print(\"llm row: \"+str(i+1))\n",
        "        i+=1\n",
        "\n",
        "print(\"# unique entities in LLM-generated dataset: \"+str(len(entities_llm)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YeEWdwr4eB-",
        "outputId": "4fdff5e1-3627-4b6f-f20f-46f764797103"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# unique entities in LLM-generated dataset: 596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute Label Accuracy"
      ],
      "metadata": {
        "id": "hAPhkSpA5XVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_correct = 0\n",
        "num_hallucinations = 0\n",
        "hallucinations = []\n",
        "\n",
        "for entity in entities_llm:\n",
        "    if entity in entities_original:\n",
        "        num_correct+=1\n",
        "    else:\n",
        "        hallucinations.append(entity)\n",
        "        num_hallucinations+=1\n",
        "print(\"\"\"Correctly labeled entities are entities that were labeled as <TYPE> entities\n",
        "in both the original devel dataset and LLM-generated datasets.\"\"\")\n",
        "print(\"# correctly labeled entities: \"+str(num_correct))\n",
        "print(\"\\n\")\n",
        "print(\"<TYPE> in the following print statements is a placeholder for entity type (i.e. chemical, disease, gene, protein)\")\n",
        "print(\"\\n\")\n",
        "print(\"\"\"Hallucinations are entities that were labeled as <TYPE> entities in the LLM-generated dataset,\n",
        "but they weren't labeled as <TYPE> entities in the original devel dataset.\"\"\")\n",
        "print(\"# hallucinations: \"+str(num_hallucinations))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Vfbm66t4leM",
        "outputId": "6158e4d7-26c5-4822-bfcf-edf399f176d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correctly labeled entities are entities that were labeled as <TYPE> entities\n",
            "in both the original devel dataset and LLM-generated datasets.\n",
            "# correctly labeled entities: 159\n",
            "\n",
            "\n",
            "<TYPE> in the following print statements is a placeholder for entity type (i.e. chemical, disease, gene, protein)\n",
            "\n",
            "\n",
            "Hallucinations are entities that were labeled as <TYPE> entities in the LLM-generated dataset,\n",
            "but they weren't labeled as <TYPE> entities in the original devel dataset.\n",
            "# hallucinations: 437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_false_neg = 0\n",
        "false_neg = []\n",
        "for entity in entities_original:\n",
        "    if entity not in entities_llm:\n",
        "        num_false_neg+=1\n",
        "        false_neg.append(entity)\n",
        "print(\"\"\"False negatives are entities that were labeled as <TYPE> entities in the original devel dataset,\n",
        "but not labeled as <TYPE> entities in the LLM-generated dataset.\"\"\")\n",
        "print(\"# false negatives: \"+str(num_false_neg))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "se7v5YLv472e",
        "outputId": "24cd8d9d-fb82-4d67-e24d-4bf6bb11678b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False negatives are entities that were labeled as <TYPE> entities in the original devel dataset,\n",
            "but not labeled as <TYPE> entities in the LLM-generated dataset.\n",
            "# false negatives: 204\n"
          ]
        }
      ]
    }
  ]
}