{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "\n",
        "This notebook is used for determining the length of human and GPT-generated BLURB dataset labels."
      ],
      "metadata": {
        "id": "Fv8XmNtp37bQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAcY_Hnk4TUT"
      },
      "source": [
        "# Env Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysb7oa_FvlJq"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "import os\n",
        "from google.colab import drive\n",
        "import time\n",
        "import ast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPo08KiRcXsx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "857e5911-59b6-46fe-8f2f-86a0b1d31a9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOOGViRbM1on",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eae3fe7e-fe56-4167-dc9d-fd5c06d33eea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1vdEcgdXIfpnlORVlPsJtHUmKXSAqr69R/6.8611 Research Project/Colab Notebooks\n"
          ]
        }
      ],
      "source": [
        "cd drive/MyDrive/'6.8611 Research Project'/'Colab Notebooks'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78oLc-uw6qb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bc6583f-879e-4eca-cbaa-d70408723611"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " BC5CDR-D_devel_1.csv                               'Intrinsic Eval Precision.ipynb'\n",
            " BC5CDR-D_devel_2.csv                                Intrinsic_exact_match.ipynb\n",
            " Data-cleaning.ipynb                                 \u001b[0m\u001b[01;34mllm-annotations\u001b[0m/\n",
            " \u001b[01;34mdevel_gpt_generated_datasets\u001b[0m/                      ' NER with BERT.ipynb'\n",
            " Entity_Intrinsic_Eval_with_aligned_datasets.ipynb   openai-test.ipynb\n",
            " Fine-Tuning-Few-Shot.ipynb                          retry_prompts.gsheet\n",
            " Fine-Tuning-Human-Annotated.ipynb                   RW-Fine-Tuning-Human-Annotated.ipynb\n",
            " Fine-Tuning-One-Shot.ipynb                          \u001b[01;34msft_training_data\u001b[0m/\n",
            " Fine-Tuning-Zero-Shot.ipynb                         TEST_LABEL_BUGS.ipynb\n",
            " GPT-Finetuning.ipynb                                tokens_labels.csv\n",
            " Intrinsic_approx_match.ipynb                        Untitled\n",
            " \u001b[01;34mintrinsic_data\u001b[0m/                                     zero-shot-bc5cdr-chem.pynb\n",
            " Intrinsic_data_clean.ipynb                         'zero_shot[FASTER].ipynb'\n",
            " intrinsic_eval_entity.ipynb                         zero-shot.pynb\n",
            " intrinsic_eval.ipynb\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70AoigM1u6Ct"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QD23yqLLb5-X"
      },
      "outputs": [],
      "source": [
        "# load the datasets into dataframes\n",
        "\n",
        "def load_tsv_dataset(file_path):\n",
        "  \"\"\"\n",
        "  Loads a tsv dataset. Renames thne columns to 'token' and 'label'.\n",
        "  Note that renaming the columns will overwrite the first row of the dataframe\n",
        "  \"\"\"\n",
        "  df = pd.read_csv(file_path, delimiter='\\t', header=None, engine='python')\n",
        "  df.columns = ['token', 'label']\n",
        "  # print(df.head())\n",
        "  return df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the datasets into dataframes\n",
        "\n",
        "def load_csv_dataset(file_path):\n",
        "  \"\"\"\n",
        "  Loads a csv dataset. Renames thne columns to 'token' and 'label'.\n",
        "  Note that renaming the columns will overwrite the first row of the dataframe\n",
        "  \"\"\"\n",
        "  df = pd.read_csv(file_path, header=None, engine='python')\n",
        "  df.columns = ['token', 'label']\n",
        "  # print(df.head())\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "jAyxxu1wp1Lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eHrO5iXiYW5"
      },
      "outputs": [],
      "source": [
        "def split_by_sentence(list_of_strings):\n",
        "  sentences = []\n",
        "  current_sentence = []\n",
        "  num_sentences = 0\n",
        "\n",
        "  for word in list_of_strings:\n",
        "      current_sentence.append(word)\n",
        "      if type(word) is str and word.endswith('.'):\n",
        "          num_sentences += 1\n",
        "          sentence_str = ' '.join(map(str, current_sentence))\n",
        "          sentences.append(sentence_str)\n",
        "          current_sentence = []\n",
        "\n",
        "  print(\"\\nNumber of sentences: \", num_sentences)\n",
        "  return sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5rzPsANcKA-"
      },
      "outputs": [],
      "source": [
        "def get_filtered_entities(df, target_label):\n",
        "  \"\"\"\n",
        "  df (pandas dataframe): has two columns 'token' and 'label'\n",
        "  target_label: 'B', 'I', or 'O' (see description above for what these signify)\n",
        "\n",
        "  Filtering involves: removing blanks, and filtering out entities that consist\n",
        "  only of punctuation, numbers, or single letters.\n",
        "\n",
        "  Return a frequency of all filtered entities with label 'target_label'.\n",
        "  \"\"\"\n",
        "  filtered_df = df[df['label'] == target_label]\n",
        "  target_entities = filtered_df['token'].tolist() # a set of all the entities with the target label\n",
        "\n",
        "  # regex for filtering out nonsense strings\n",
        "  punctuation = re.escape(string.punctuation)\n",
        "  pattern = re.compile(rf'^(?![a-zA-Z]?$)(?!\\d+$)(?!^[{punctuation}]+$).+')\n",
        "  target_entities = [ent for ent in target_entities if pattern.match(ent)]\n",
        "  return Counter(target_entities)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intrinsic Eval\n"
      ],
      "metadata": {
        "id": "_LHyMze2Rwx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dfs(dataset):\n",
        "  print('\\nFor ', dataset, \": \\n\")\n",
        "  zero_shot = f'devel_gpt_generated_datasets/zero_shot/{dataset}-devel.csv'\n",
        "  one_shot = f'devel_gpt_generated_datasets/one_shot/{dataset}-devel.csv'\n",
        "  few_shot = f'devel_gpt_generated_datasets/few_shot/{dataset}-devel.csv'\n",
        "  devel = f'llm-annotations/datasets/{dataset}/devel.tsv'\n",
        "\n",
        "  zero_shot_df = load_csv_dataset(zero_shot)\n",
        "  one_shot_df = load_csv_dataset(one_shot)\n",
        "  few_shot_df = load_csv_dataset(few_shot)\n",
        "  devel_df = load_tsv_dataset(devel)\n",
        "  print('Zero Shot Length: ', len(zero_shot_df), '\\nOne Shot Length: ', len(one_shot_df), '\\nFew Shot Length: ', len(few_shot_df), '\\nTrue Length: ', len(devel_df))\n",
        "\n",
        "  return devel_df, zero_shot_df, one_shot_df, few_shot_df"
      ],
      "metadata": {
        "id": "t5TC5GQk8o0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_shot = f'devel_gpt_generated_datasets/one_shot/BC5CDR-disease-devel.csv'\n",
        "one_shot_df = load_csv_dataset(one_shot)"
      ],
      "metadata": {
        "id": "-c_Up3t3qWeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_chunks(all_tokens, CHUNK_SIZE = 300):\n",
        "  nans = 0\n",
        "  for i in range(len(all_tokens)):\n",
        "    token = all_tokens[i]\n",
        "    if isinstance(token,float) and str(token)==\"nan\":\n",
        "      nans +=1\n",
        "      all_tokens[i]=\"null\"\n",
        "  print('Number of nans: ', nans)\n",
        "\n",
        "  sentences = split_by_sentence(all_tokens)\n",
        "\n",
        "  SENTENCE_CHUNKS = []\n",
        "  sentence_chunk_len = []\n",
        "  word_chunk_len = []\n",
        "  chunk_sentence_count = 0\n",
        "\n",
        "  curr_chunk, curr_chunk_len = [], 0\n",
        "  for sent in sentences:\n",
        "    curr_chunk.append(sent)\n",
        "    curr_chunk_len += len(sent)\n",
        "    chunk_sentence_count += 1\n",
        "    if curr_chunk_len >= CHUNK_SIZE:\n",
        "      word_chunk_len.append(curr_chunk_len)\n",
        "      sentence_chunk_len.append(chunk_sentence_count)\n",
        "      SENTENCE_CHUNKS.append(' '.join(curr_chunk))\n",
        "      curr_chunk = []\n",
        "      curr_chunk_len = 0\n",
        "      chunk_sentence_count = 0\n",
        "\n",
        "  print('Number of Sentences in First 10 Chunks: ', sentence_chunk_len[:10])\n",
        "  SENTENCE_CHUNKS.append(' '.join(curr_chunk))\n",
        "  print('Number of Chunks: ', len(SENTENCE_CHUNKS))\n",
        "  return SENTENCE_CHUNKS, sentence_chunk_len, word_chunk_len"
      ],
      "metadata": {
        "id": "yitqFHkSR1iL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_label_dict(dataset_df, name):\n",
        "  print('\\nFor', name, ':')\n",
        "  all_tokens = dataset_df['token'].tolist()\n",
        "  CHUNK_SIZE = 300 # string length of the chunk\n",
        "  sentence_chunks, sentence_chunk_len, word_chunk_len = get_chunks(all_tokens)\n",
        "\n",
        "  n = len(dataset_df)\n",
        "  i = 0\n",
        "\n",
        "  sentence_labels = []\n",
        "\n",
        "  while i < n:\n",
        "    for chunk in sentence_chunks:\n",
        "      num_tokens = len(chunk.split())\n",
        "      rows = dataset_df.iloc[i:i+num_tokens].reset_index()\n",
        "\n",
        "      tokens = [str(row['token'])+ '_' + str(i) for i, row in rows.iterrows()]\n",
        "      labels = [row['label'] for _, row in rows.iterrows()]\n",
        "\n",
        "      sentence_labels.append(dict(zip(tokens, labels)))\n",
        "\n",
        "      i += num_tokens\n",
        "  return all_tokens, sentence_chunks, sentence_labels, sentence_chunk_len, word_chunk_len\n"
      ],
      "metadata": {
        "id": "sVcaP0TEj3tP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_length_analysis(length_type, devel_chunk_lengths, zero_chunk_lengths, one_chunk_lengths, few_chunk_lengths):\n",
        "  d_z_same = True\n",
        "  d_o_same = True\n",
        "\n",
        "  for i in range(len(devel_chunk_lengths)):\n",
        "    if d_z_same and devel_chunk_lengths[i] != zero_chunk_lengths[i] or d_o_same and devel_chunk_lengths[i] != one_chunk_lengths[i]:\n",
        "      if d_z_same and devel_chunk_lengths[i] != zero_chunk_lengths[i]:\n",
        "        d_z_same = False\n",
        "        print('Number of ', length_type, ' in devel and ZERO shot chunks are identical until chunk number: ', i)\n",
        "\n",
        "      if d_o_same and devel_chunk_lengths[i] != one_chunk_lengths[i]:\n",
        "        d_o_same = False\n",
        "        print('Number of ', length_type, ' in devel and ONE shot chunks are identical until chunk number: ', i)\n",
        "\n",
        "      continue"
      ],
      "metadata": {
        "id": "xxmK07sL5_yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for dataset in ['NCBI', 'JNLPBA', 'BC5CDR-chem', 'BC5CDR-disease', 'BC2GM']:\n",
        "  devel, zero_shot, one_shot, few_shot = load_dfs(dataset)\n",
        "\n",
        "  # for df, name in [[devel, 'devel'], [zero_shot, 'zero_shot'], [one_shot, 'one_shot']]:\n",
        "  #   print(name, 'has ', get_filtered_entities(df, 'B'), ' entities labeled B, ', get_filtered_entities(df, 'I'), ' entities labeled I, and', get_filtered_entities(df, 'O'), ' entities labeled O')\n",
        "\n",
        "\n",
        "  devel_tokens, devel_chunks, devel_labels, devel_chunk_s_lengths, devel_chunk_w_lengths = data_label_dict(devel, dataset + '_human')\n",
        "  zero_tokens, zero_chunks, zero_labels, zero_chunk_s_lengths, zero_chunk_w_lengths = data_label_dict(zero_shot, dataset + '_zeroshot')\n",
        "  one_tokens, one_chunks, one_labels, one_chunk_s_lengths, one_chunk_w_lengths = data_label_dict(one_shot, dataset + '_oneshot')\n",
        "  few_tokens, few_chunks, few_labels, few_chunk_s_lengths, few_chunk_w_lengths = data_label_dict(few_shot, dataset + '_fewshot')\n",
        "\n",
        "  chunk_length_analysis('sentences', devel_chunk_s_lengths, zero_chunk_s_lengths, one_chunk_s_lengths, few_chunk_s_lengths)\n",
        "  chunk_length_analysis('words', devel_chunk_w_lengths, zero_chunk_w_lengths, one_chunk_w_lengths, few_chunk_w_lengths)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kKvPS8qfwQtP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "outputId": "42ecf3c0-36f7-468c-e770-258a58a5c466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "For  NCBI : \n",
            "\n",
            "Zero Shot Length:  23880 \n",
            "One Shot Length:  23924 \n",
            "Few Shot Length:  23916 \n",
            "True Length:  23965\n",
            "\n",
            "For NCBI_human :\n",
            "Number of nans:  9\n",
            "\n",
            "Number of sentences:  1027\n",
            "Number of Sentences in First 10 Chunks:  [4, 3, 5, 3, 3, 4, 3, 3, 4, 3]\n",
            "Number of Chunks:  348\n",
            "\n",
            "For NCBI_zeroshot :\n",
            "Number of nans:  10\n",
            "\n",
            "Number of sentences:  933\n",
            "Number of Sentences in First 10 Chunks:  [4, 3, 4, 3, 3, 3, 2, 3, 4, 3]\n",
            "Number of Chunks:  350\n",
            "\n",
            "For NCBI_oneshot :\n",
            "Number of nans:  9\n",
            "\n",
            "Number of sentences:  963\n",
            "Number of Sentences in First 10 Chunks:  [4, 3, 4, 3, 3, 4, 3, 3, 4, 3]\n",
            "Number of Chunks:  349\n",
            "\n",
            "For NCBI_fewshot :\n",
            "Number of nans:  9\n",
            "\n",
            "Number of sentences:  957\n",
            "Number of Sentences in First 10 Chunks:  [4, 3, 4, 3, 3, 4, 2, 3, 4, 3]\n",
            "Number of Chunks:  346\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-2f6ca34aa9ed>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mfew_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfew_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfew_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfew_chunk_s_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfew_chunk_w_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_label_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfew_shot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_fewshot'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mchunk_length_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevel_chunk_s_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_chunk_s_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_chunk_s_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfew_chunk_s_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0mchunk_length_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'words'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevel_chunk_w_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_chunk_w_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_chunk_w_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfew_chunk_w_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: chunk_length_analysis() takes 4 positional arguments but 5 were given"
          ]
        }
      ]
    }
  ]
}